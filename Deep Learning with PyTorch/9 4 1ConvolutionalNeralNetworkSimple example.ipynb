{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n",
    "<h1 align=center><font size = 5>Convolutional Neural Network Simple example </font></h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Objective for this Notebook<h3>    \n",
    "<h5> 1. Learn Convolutional Neural Network</h5>\n",
    "<h5> 2. Define Softmax, Criterion function, Optimizer and Train the  Model</h5>    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "In this lab, we will use a Convolutional Neural Networks to classify horizontal an vertical Lines \n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<li><a href=\"#ref0\">Helper functions </a></li>\n",
    "<li><a href=\"#ref1\"> Prepare Data </a></li>\n",
    "<li><a href=\"#ref2\">Build a Convolutional Neural Network Class </a></li>\n",
    "<li><a href=\"#ref3\">Define the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the  Model</a></li>\n",
    "<li><a href=\"#ref4\">Analyse Results</a></li>\n",
    "\n",
    "<br>\n",
    "<p></p>\n",
    "Estimated Time Needed: <strong>25 min</strong>\n",
    "</div>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref0\"></a>\n",
    "<a name=\"ref0\"><h2 align=center>Helper functions </h2></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x72aa8c22fc90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to plot out the parameters of the Convolutional layers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_channels(W):\n",
    "    #number of output channels \n",
    "    n_out=W.shape[0]\n",
    "    #number of input channels \n",
    "    n_in=W.shape[1]\n",
    "    w_min=W.min().item()\n",
    "    w_max=W.max().item()\n",
    "    fig, axes = plt.subplots(n_out,n_in)\n",
    "    fig.subplots_adjust(hspace = 0.1)\n",
    "    out_index=0\n",
    "    in_index=0\n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "    \n",
    "        if in_index>n_in-1:\n",
    "            out_index=out_index+1\n",
    "            in_index=0\n",
    "              \n",
    "        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index=in_index+1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>show_data</code>: plot out data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_data(dataset,sample):\n",
    "\n",
    "    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n",
    "    plt.title('y='+str(dataset.y[sample].item()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create some toy data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n",
    "        \"\"\"\n",
    "        p:portability that pixel is wight  \n",
    "        N_images:number of images \n",
    "        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n",
    "        \"\"\"\n",
    "        if train==True:\n",
    "            np.random.seed(1)  \n",
    "        \n",
    "        #make images multiple of 3 \n",
    "        N_images=2*(N_images//2)\n",
    "        images=np.zeros((N_images,1,11,11))\n",
    "        start1=3\n",
    "        start2=1\n",
    "        self.y=torch.zeros(N_images).type(torch.long)\n",
    "\n",
    "        for n in range(N_images):\n",
    "            if offset>0:\n",
    "        \n",
    "                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n",
    "                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n",
    "            else:\n",
    "                low=4\n",
    "                high=1\n",
    "        \n",
    "            if n<=N_images//2:\n",
    "                self.y[n]=0\n",
    "                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n",
    "            elif  n>N_images//2:\n",
    "                self.y[n]=1\n",
    "                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n",
    "           \n",
    "        \n",
    "        \n",
    "        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        self.len=self.x.shape[0]\n",
    "        del(images)\n",
    "        np.random.seed(0)\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>plot_activation</code>: plot out the activations of the Convolutional layers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_activations(A,number_rows= 1,name=\"\"):\n",
    "    A=A[0,:,:,:].detach().numpy()\n",
    "    n_activations=A.shape[0]\n",
    "    \n",
    "    \n",
    "    print(n_activations)\n",
    "    A_min=A.min().item()\n",
    "    A_max=A.max().item()\n",
    "\n",
    "    if n_activations==1:\n",
    "\n",
    "        # Plot the image.\n",
    "        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n",
    "        fig.subplots_adjust(hspace = 0.4)\n",
    "        for i,ax in enumerate(axes.flat):\n",
    "            if i< n_activations:\n",
    "                # Set the label for the sub-plot.\n",
    "                ax.set_xlabel( \"activation:{0}\".format(i+1))\n",
    "\n",
    "                # Plot the image.\n",
    "                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Utility function for computing output of convolutions\n",
    "takes a tuple of (h,w) and returns a tuple of (h,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    #by Duane Nielsen\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "<a name=\"ref1\"><h2 align=center>Prepare Data </h2></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset with 10000 samples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_images=10000\n",
    "train_dataset=Data(N_images=N_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Data at 0x72aa3ea7ed50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset=Data(N_images=1000,train=False)\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the data type is long \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the third label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the 3rd  sample \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref2\"></a>\n",
    "<a name=\"ref2\"><h2 align=center>Build a Convolutional Neural Network Class </h2></a> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input image is 11 x11, the following will change the size of the activations:\n",
    "<ul>\n",
    "<il>convolutional layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>convolutional layer </il>\n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer </il>\n",
    "</ul>\n",
    "\n",
    "with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\n",
    "We use the following  lines of code to change the image before we get tot he fully connected layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(9, 9)\n",
      "(8, 8)\n",
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out)\n",
    "out1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out1)\n",
    "out2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out2)\n",
    "\n",
    "out3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,out_1=2,out_2=1):\n",
    "        \n",
    "        super(CNN,self).__init__()\n",
    "        #first Convolutional layers \n",
    "        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "\n",
    "        #second Convolutional layers\n",
    "        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "        #max pooling \n",
    "\n",
    "        #fully connected layer \n",
    "        self.fc1=nn.Linear(out_2*7*7,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn1(x)\n",
    "        #activation function \n",
    "        x=torch.relu(x)\n",
    "        #max pooling \n",
    "        x=self.maxpool1(x)\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn2(x)\n",
    "        #activation function\n",
    "        x=torch.relu(x)\n",
    "        #max pooling\n",
    "        x=self.maxpool2(x)\n",
    "        #flatten output \n",
    "        x=x.view(x.size(0),-1)\n",
    "        #fully connected layer\n",
    "        x=self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def activations(self,x):\n",
    "        #outputs activation this is not necessary just for fun \n",
    "        z1=self.cnn1(x)\n",
    "        a1=torch.relu(z1)\n",
    "        out=self.maxpool1(a1)\n",
    "        \n",
    "        z2=self.cnn2(out)\n",
    "        a2=torch.relu(z2)\n",
    "        out=self.maxpool2(a2)\n",
    "        out=out.view(out.size(0),-1)\n",
    "        return z1,a1,z2,a2,out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "<a name=\"ref3\"><h2> Define the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the  Model</h2></a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 output channels for the first layer, and 1 outputs channel for the second layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=CNN(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the model parameters with the object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (cnn1): Conv2d(1, 2, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2): Conv2d(2, 1, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=49, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_channels(model.state_dict()['cnn1.weight'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " optimizer class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\n",
    "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "N_test=len(validation_dataset)\n",
    "cost=0\n",
    "#n_epochs\n",
    "for epoch in range(n_epochs):\n",
    "    cost=0    \n",
    "    for x, y in train_loader:\n",
    "      \n",
    "\n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z=model(x)\n",
    "        # calculate loss \n",
    "        loss=criterion(z,y)\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        cost+=loss.item()\n",
    "    cost_list.append(cost)\n",
    "        \n",
    "        \n",
    "    correct=0\n",
    "    #perform a prediction on the validation  data  \n",
    "    for x_test, y_test in validation_loader:\n",
    "\n",
    "        z=model(x_test)\n",
    "        _,yhat=torch.max(z.data,1)\n",
    "\n",
    "        correct+=(yhat==y_test).sum().item()\n",
    "        \n",
    "\n",
    "    accuracy=correct/N_test\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"ref4\"></a>\n",
    "<a name=\"ref4\"><h2 align=center>Analyse Results</h2></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss and accuracy on the validation data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(cost_list,color=color)\n",
    "ax1.set_xlabel('epoch',color=color)\n",
    "ax1.set_ylabel('total loss',color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  \n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results of the parameters for the Convolutional layers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the activations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\n",
    "out=model.activations(train_dataset[0][0].view(1,1,11,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot them out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[0],number_rows=1,name=\" feature map\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[3],number_rows=1,name=\"first feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we save the output of the activation after flattening  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1=out[4][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can do the same for a sample  where y=0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\n",
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot( out1, 'b')\n",
    "plt.title('Flatted Activation Values  ')\n",
    "plt.ylabel('Activation')\n",
    "plt.xlabel('index')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(out0, 'r')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Authors:  \n",
    "[Joseph Santarcangelo]( https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n",
    "\n",
    "Other contributors: [Michelle Carey](  https://www.linkedin.com/in/michelleccarey/) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "-->\n",
    "\n",
    "## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "a52c626bae0836e780d9fc3789d4a7038a36aa68d3201241c323a06b7dd54d25"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
