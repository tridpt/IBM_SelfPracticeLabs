{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Implementing Q-Learning in Keras**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Overview \n",
    "In this lab, you will implement a Q-Learning algorithm using Keras to solve a reinforcement learning problem.\n",
    "\n",
    "## Learning objectives:\n",
    "By the end of this lab, you will:  \n",
    "- Implement a Q-Learning algorithm using Keras\n",
    "- Define and train a neural network to approximate the Q-values\n",
    "- Evaluate the performance of the trained Q-Learning agent\n",
    "\n",
    "## Prerequisites \n",
    "- Basic knowledge of Python programming\n",
    "- Familiarity with Keras and neural networks\n",
    "- Understanding of reinforcement learning concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Guide \n",
    "\n",
    "#### Step 1: Setting Up the Environment \n",
    "\n",
    "First, you will set up the environment using the OpenAI Gym library. You will use the 'CartPole-v1' environment, a common benchmark for reinforcement learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy>=1.18.0 (from gym)\n",
      "  Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting cloudpickle>=1.2.0 (from gym)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading gym_notices-0.1.0-py3-none-any.whl (3.3 kB)\n",
      "Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m162.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?2done\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827730 sha256=85cd072a3f6e75784818502bbbbdff1405a3bce0e8e601836d173b0598462bde\n",
      "  Stored in directory: /home/jupyterlab/.cache/pip/wheels/95/51/6c/9bb05ebbe7c5cb8171dfaa3611f32622ca4658d53f31c79077\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, numpy, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.1.1 gym-0.26.2 gym_notices-0.1.0 numpy-2.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m174.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.2\n",
      "    Uninstalling numpy-2.3.2:\n",
      "      Successfully uninstalled numpy-2.3.2\n",
      "Successfully installed numpy-1.26.4\n",
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "Collecting tensorflow==2.16.2\n",
      "  Downloading tensorflow-2.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.16.2)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.16.2)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow==2.16.2)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.16.2)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.16.2)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow==2.16.2)\n",
      "  Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.16.2)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.2)\n",
      "  Downloading ml_dtypes-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.16.2)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.16.2)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.16.2)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow==2.16.2)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.16.2)\n",
      "  Downloading grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.2)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow==2.16.2)\n",
      "  Downloading keras-3.11.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.16.2) (0.45.1)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow==2.16.2)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow==2.16.2)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow==2.16.2)\n",
      "  Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow==2.16.2)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow==2.16.2)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow==2.16.2)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow==2.16.2)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (590.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.8/590.8 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.11.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.74.0 h5py-3.14.0 keras-3.11.1 libclang-18.1.1 markdown-3.8.2 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.1.0 opt-einsum-3.4.0 optree-0.17.0 protobuf-4.25.8 rich-14.1.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.2 termcolor-3.1.0 werkzeug-3.1.3 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy==1.26.4\n",
    "!pip uninstall tensorflow -y \n",
    "!pip install tensorflow==2.16.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment Variables \n",
    "Sometimes, environment variables can help mitigate certain issues with TensorFlow. You can try disabling the oneDNN optimizations or CUDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Recursion Limit \n",
    "You can also try increasing the recursion limit, although this is generally more of a workaround than a solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.setrecursionlimit(1500) \n",
    "\n",
    "import gym \n",
    "import numpy as np \n",
    "\n",
    "# Create the environment \n",
    "env = gym.make('CartPole-v1') \n",
    "\n",
    "# Set random seed for reproducibility \n",
    "np.random.seed(42) \n",
    "env.action_space.seed(42) \n",
    "env.observation_space.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:  \n",
    "- `gym` is a toolkit for developing and comparing reinforcement learning algorithms.\n",
    "- `CartPole-v1` is an environment where a pole is balanced on a cart, and the goal is to prevent the pole from falling over.\n",
    "- Setting random seeds ensures that you can reproduce the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Q-Learning Model \n",
    "\n",
    "You will define a neural network using Keras to approximate the Q-values. The network will take the state as input and output Q-values for each action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 02:21:14.176354: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-09 02:21:14.177685: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-09 02:21:14.182701: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-09 02:21:14.196025: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-09 02:21:14.223234: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-09 02:21:14.223292: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-09 02:21:14.240485: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-09 02:21:15.256099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings for a cleaner notebook or console experience\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Override the default warning function\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "\n",
    "# Import necessary libraries for the Q-Learning model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input  # Import Input layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym  # Ensure the environment library is available\n",
    "\n",
    "# Define the model building function\n",
    "def build_model(state_size, action_size): \n",
    "    model = Sequential() \n",
    "    model.add(Input(shape=(state_size,)))  # Use Input layer to specify the input shape \n",
    "    model.add(Dense(24, activation='relu')) \n",
    "    model.add(Dense(24, activation='relu')) \n",
    "    model.add(Dense(action_size, activation='linear')) \n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001)) \n",
    "    return model \n",
    "\n",
    "# Create the environment and set up the model\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0] \n",
    "action_size = env.action_space.n \n",
    "model = build_model(state_size, action_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: \n",
    "- `Sequential` model: a linear stack of layers in Keras. \n",
    "- `Dense` layers: fully connected layers. \n",
    "- `input_dim`: the size of the input layer, corresponding to the state size. \n",
    "- `activation='relu'`: Rectified Linear Unit activation function. \n",
    "- `activation='linear'`: linear activation function for the output layer, as we are predicting continuous Q-values. \n",
    "- `Adam` optimizer: an optimization algorithm that adjusts the learning rate based on gradients. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Implement the Q-Learning Algorithm \n",
    "\n",
    "Now, you will implement the Q-Learning algorithm, which involves interacting with the environment, updating the Q-values, and training the neural network. \n",
    "\n",
    "**Define the replay Function**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/10, score: 21, e: 1.0\n",
      "episode: 2/10, score: 20, e: 1.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "episode: 3/10, score: 23, e: 0.99\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "episode: 4/10, score: 41, e: 0.9\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "episode: 5/10, score: 21, e: 0.86\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "episode: 6/10, score: 13, e: 0.83\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "episode: 7/10, score: 41, e: 0.76\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "episode: 8/10, score: 21, e: 0.72\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "episode: 9/10, score: 10, e: 0.71\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "episode: 10/10, score: 15, e: 0.69\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define epsilon and epsilon_decay\n",
    "epsilon = 1.0  # Starting with a high exploration rate\n",
    "epsilon_min = 0.01  # Minimum exploration rate\n",
    "epsilon_decay = 0.99  # Faster decay rate for epsilon after each episode\n",
    "\n",
    "# Replay memory\n",
    "memory = deque(maxlen=2000)\n",
    "\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    \"\"\"Store experience in memory.\"\"\"\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def replay(batch_size=64):  # Increased batch size\n",
    "    \"\"\"Train the model using a random sample of experiences from memory.\"\"\"\n",
    "    if len(memory) < batch_size:\n",
    "        return  # Skip replay if there's not enough experience\n",
    "\n",
    "    minibatch = random.sample(memory, batch_size)  # Sample a random batch from memory\n",
    "    \n",
    "    # Extract information for batch processing\n",
    "    states = np.vstack([x[0] for x in minibatch])\n",
    "    actions = np.array([x[1] for x in minibatch])\n",
    "    rewards = np.array([x[2] for x in minibatch])\n",
    "    next_states = np.vstack([x[3] for x in minibatch])\n",
    "    dones = np.array([x[4] for x in minibatch])\n",
    "    \n",
    "    # Predict Q-values for the next states in batch\n",
    "    q_next = model.predict(next_states)\n",
    "    # Predict Q-values for the current states in batch\n",
    "    q_target = model.predict(states)\n",
    "    \n",
    "    # Vectorized update of target values\n",
    "    for i in range(batch_size):\n",
    "        target = rewards[i]\n",
    "        if not dones[i]:\n",
    "            target += 0.95 * np.amax(q_next[i])  # Update Q value with the discounted future reward\n",
    "        q_target[i][actions[i]] = target  # Update only the taken action's Q value\n",
    "    \n",
    "    # Train the model with the updated targets in batch\n",
    "    model.fit(states, q_target, epochs=1, verbose=0)  # Train in batch mode\n",
    "\n",
    "    # Reduce exploration rate (epsilon) after each training step\n",
    "    global epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "def act(state):\n",
    "    \"\"\"Choose an action based on the current state and exploration rate.\"\"\"\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size)  # Explore: choose a random action\n",
    "    act_values = model.predict(state)  # Exploit: predict action based on the state\n",
    "    return np.argmax(act_values[0])  # Return the action with the highest Q-value\n",
    "\n",
    "# Define the number of episodes you want to train the model for\n",
    "episodes = 10  # You can set this to any number you prefer\n",
    "train_frequency = 5  # Train the model every 5 steps\n",
    "\n",
    "for e in range(episodes):\n",
    "    state, _ = env.reset()  # Unpack the tuple returned by env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(200):  # Limit to 200 time steps per episode\n",
    "        action = act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        remember(state, action, reward, next_state, done)  # Store experience\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(f\"episode: {e+1}/{episodes}, score: {time}, e: {epsilon:.2}\")\n",
    "            break\n",
    "        \n",
    "        # Train the model every 'train_frequency' steps\n",
    "        if time % train_frequency == 0:\n",
    "            replay(batch_size=64)  # Call replay with larger batch size for efficiency\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Evaluate the Performance \n",
    "\n",
    "Finally, you will evaluate the performance of the trained Q-Learning agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(10):  \n",
    "\n",
    "    state, _ = env.reset()  # Unpack the state from the tuple \n",
    "    state = np.reshape(state, [1, state_size])  # Reshape the state correctly \n",
    "    for time in range(500):  \n",
    "        env.render()  \n",
    "        action = np.argmax(model.predict(state)[0])  \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)  # Unpack the five return values \n",
    "        done = terminated or truncated  # Check if the episode is done \n",
    "        next_state = np.reshape(next_state, [1, state_size])  \n",
    "        state = next_state  \n",
    "        if done:  \n",
    "            print(f\"episode: {e+1}/10, score: {time}\")  \n",
    "            break  \n",
    "\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: \n",
    "- This loop runs 10 episodes to test the trained agent. \n",
    "- `env.render()`: visualizes the environment. \n",
    "- The agent chooses actions based on the trained model and interacts with the environment. \n",
    "- The score for each episode is printed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice exercises \n",
    "\n",
    "## Exercise 1: Experiment with Different Network Architectures \n",
    "\n",
    "### Objective: \n",
    "Understand how changing the architecture of the neural network affects the performance of the Q-Learning agent. \n",
    "\n",
    "### Instructions: \n",
    "1. Modify the `build_model()` function to include a different number of neurons and layers. For example, increase the number of layers to 3 and the number of neurons in each layer to 64. \n",
    "2. Train the model with the modified architecture and observe the performance in terms of average score achieved over 100 episodes. \n",
    "3. Compare the performance with the original architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# Install gym if necessary\n",
    "!pip install gym\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Global settings\n",
    "episodes = 10  # Number of episodes\n",
    "batch_size = 32  # Size of the mini-batch for training\n",
    "memory = deque(maxlen=2000)  # Memory buffer to store experiences\n",
    "\n",
    "# Define state size and action size based on the environment\n",
    "state_size = env.observation_space.shape[0]  # State space size from the environment\n",
    "action_size = env.action_space.n  # Number of possible actions from the environment\n",
    "\n",
    "# Define the model\n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(state_size,)))  # Explicit Input layer\n",
    "    model.add(Dense(32, activation='relu'))  # Smaller hidden layers\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# Re-initialize the model with the new architecture\n",
    "model = build_model(state_size, action_size)\n",
    "\n",
    "# Placeholder for your action function (e.g., epsilon-greedy)\n",
    "def act(state):\n",
    "    return env.action_space.sample()  # For now, a random action is taken\n",
    "\n",
    "# Function to remember experiences in memory\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "# Optimized function to replay experiences from memory and train the model\n",
    "def replay(batch_size):\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    states = np.vstack([sample[0] for sample in minibatch])\n",
    "    next_states = np.vstack([sample[3] for sample in minibatch])\n",
    "    targets = model.predict(states)\n",
    "    target_next = model.predict(next_states)\n",
    "    \n",
    "    for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "        target = reward if done else reward + 0.95 * np.amax(target_next[i])\n",
    "        targets[i][action] = target\n",
    "        \n",
    "    model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "# Train the model with the modified architecture\n",
    "for e in range(episodes):\n",
    "    state, _ = env.reset()  # Unpack the state from the tuple\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(200):  # Reduced number of steps per episode\n",
    "        action = act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(f\"episode: {e+1}/{episodes}, score: {time}\")\n",
    "            break\n",
    "        \n",
    "        if len(memory) > batch_size and time % 10 == 0:  # Train every 10 steps\n",
    "            replay(batch_size)  # Pass the batch size to replay()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Install gym if necessary\n",
    "!pip install gym\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Global settings\n",
    "episodes = 10  # Number of episodes\n",
    "batch_size = 32  # Size of the mini-batch for training\n",
    "memory = deque(maxlen=2000)  # Memory buffer to store experiences\n",
    "\n",
    "# Define state size and action size based on the environment\n",
    "state_size = env.observation_space.shape[0]  # State space size from the environment\n",
    "action_size = env.action_space.n  # Number of possible actions from the environment\n",
    "\n",
    "# Define the model\n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(state_size,)))  # Explicit Input layer\n",
    "    model.add(Dense(32, activation='relu'))  # Smaller hidden layers\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# Re-initialize the model with the new architecture\n",
    "model = build_model(state_size, action_size)\n",
    "\n",
    "# Placeholder for your action function (e.g., epsilon-greedy)\n",
    "def act(state):\n",
    "    return env.action_space.sample()  # For now, a random action is taken\n",
    "\n",
    "# Function to remember experiences in memory\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "# Optimized function to replay experiences from memory and train the model\n",
    "def replay(batch_size):\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    states = np.vstack([sample[0] for sample in minibatch])\n",
    "    next_states = np.vstack([sample[3] for sample in minibatch])\n",
    "    targets = model.predict(states)\n",
    "    target_next = model.predict(next_states)\n",
    "    \n",
    "    for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "        target = reward if done else reward + 0.95 * np.amax(target_next[i])\n",
    "        targets[i][action] = target\n",
    "        \n",
    "    model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "# Train the model with the modified architecture\n",
    "for e in range(episodes):\n",
    "    state, _ = env.reset()  # Unpack the state from the tuple\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(200):  # Reduced number of steps per episode\n",
    "        action = act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(f\"episode: {e+1}/{episodes}, score: {time}\")\n",
    "            break\n",
    "        \n",
    "        if len(memory) > batch_size and time % 10 == 0:  # Train every 10 steps\n",
    "            replay(batch_size)  # Pass the batch size to replay()\n",
    "\n",
    "env.close()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement an Adaptive Exploration Rate \n",
    "\n",
    "### Objective: \n",
    "Learn how to adapt the exploration rate (`epsilon`) based on the agent's performance to balance exploration and exploitation. \n",
    "\n",
    "### Instructions: \n",
    "1. Modify the `epsilon` decay strategy to decrease more rapidly when the agent's performance improves significantly. \n",
    "2. Implement a check to reduce `epsilon` faster if the agent achieves a score greater than a certain threshold (e.g., 200) in consecutive episodes. \n",
    "3. Observe the effect on the learning rate and the agent's performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# Function to adjust epsilon based on performance\n",
    "def adjust_epsilon(score, consecutive_success_threshold=200):\n",
    "    global epsilon \n",
    "\n",
    "    if score >= consecutive_success_threshold: \n",
    "        epsilon = max(epsilon_min, epsilon * 0.9)  # Reduce epsilon faster if performance is good\n",
    "    else: \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)  # Regular epsilon decay\n",
    "\n",
    "episodes = 2  # Set number of episodes for training\n",
    "\n",
    "# Train the model with adaptive epsilon decay\n",
    "for e in range(episodes): \n",
    "    state = env.reset()  \n",
    "    state = state[0]  # Extract the first element, which is the actual state array\n",
    "    state = np.reshape(state, [1, len(state)])  # Reshape state to match the expected input shape\n",
    "\n",
    "    total_reward = 0 \n",
    "\n",
    "    for time in range(500):  # Limit the episode to 500 time steps\n",
    "        action = act(state)  # Choose action based on policy\n",
    "        next_state, reward, done, truncated, _ = env.step(action)  # Unpack 5 values\n",
    "\n",
    "        reward = reward if not done else -10  # Penalize for reaching a terminal state\n",
    "        total_reward += reward  # Accumulate rewards\n",
    "\n",
    "        next_state = np.reshape(next_state, [1, len(next_state)])  # Reshape next state (optional based on model needs)\n",
    "\n",
    "        remember(state, action, reward, next_state, done)  # Store experience in memory\n",
    "        state = next_state  # Update the current state\n",
    "\n",
    "        if done or truncated:  # Check if the episode is done or truncated\n",
    "            adjust_epsilon(total_reward)  # Adjust epsilon based on the total reward\n",
    "            print(f\"episode: {e}/{episodes}, score: {time}, e: {epsilon:.2}\")  # Print the episode details\n",
    "            break  # Break out of the loop if the episode is done or truncated\n",
    "\n",
    "        if len(memory) > batch_size:  # Check if enough experiences are stored in memory\n",
    "            replay(batch_size)  # Train the model with the stored experiences (pass batch_size here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Function to adjust epsilon based on performance\n",
    "def adjust_epsilon(score, consecutive_success_threshold=200):\n",
    "    global epsilon \n",
    "\n",
    "    if score >= consecutive_success_threshold: \n",
    "        epsilon = max(epsilon_min, epsilon * 0.9)  # Reduce epsilon faster if performance is good\n",
    "    else: \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)  # Regular epsilon decay\n",
    "\n",
    "episodes = 2  # Set number of episodes for training\n",
    "\n",
    "# Train the model with adaptive epsilon decay\n",
    "for e in range(episodes): \n",
    "    state = env.reset()  \n",
    "    state = state[0]  # Extract the first element, which is the actual state array\n",
    "    state = np.reshape(state, [1, len(state)])  # Reshape state to match the expected input shape\n",
    "\n",
    "    total_reward = 0 \n",
    "\n",
    "    for time in range(500):  # Limit the episode to 500 time steps\n",
    "        action = act(state)  # Choose action based on policy\n",
    "        next_state, reward, done, truncated, _ = env.step(action)  # Unpack 5 values\n",
    "\n",
    "        reward = reward if not done else -10  # Penalize for reaching a terminal state\n",
    "        total_reward += reward  # Accumulate rewards\n",
    "\n",
    "        next_state = np.reshape(next_state, [1, len(next_state)])  # Reshape next state (optional based on model needs)\n",
    "\n",
    "        remember(state, action, reward, next_state, done)  # Store experience in memory\n",
    "        state = next_state  # Update the current state\n",
    "\n",
    "        if done or truncated:  # Check if the episode is done or truncated\n",
    "            adjust_epsilon(total_reward)  # Adjust epsilon based on the total reward\n",
    "            print(f\"episode: {e}/{episodes}, score: {time}, e: {epsilon:.2}\")  # Print the episode details\n",
    "            break  # Break out of the loop if the episode is done or truncated\n",
    "\n",
    "        if len(memory) > batch_size:  # Check if enough experiences are stored in memory\n",
    "            replay(batch_size)  # Train the model with the stored experiences (pass batch_size here)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 : Implement a Custom Reward Function \n",
    "\n",
    "### Objective: \n",
    "Understand the impact of reward shaping on training the Q-Learning agent. \n",
    "\n",
    "### Instructions: \n",
    "1. Modify the reward function to provide more granular feedback to the agent. For example, give higher rewards for keeping the pole more vertical and closer to the center. \n",
    "2. Implement a reward function that rewards the agent proportionally to the angle of the pole and the distance of the cart from the center. \n",
    "3. Train the agent with the new reward function and compare the learning speed and stability to the original setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom reward function based on the cart position and pole angle\n",
    "def custom_reward(state):\n",
    "    # Extract state variables: x (cart position), x_dot (cart velocity), theta (pole angle), theta_dot (pole angular velocity)\n",
    "    x, x_dot, theta, theta_dot = state\n",
    "    \n",
    "    # Custom reward function: Encourage the agent to keep the cart near the center and the pole upright\n",
    "    reward = (1 - abs(x) / 2.4) + (1 - abs(theta) / 0.20948)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "episodes = 2  # Number of episodes to run\n",
    "\n",
    "# Train the model with the custom reward function\n",
    "for e in range(episodes): \n",
    "    state = env.reset()  # Reset the environment\n",
    "\n",
    "    # Print the state structure for debugging\n",
    "    print(f\"State: {state}, State Type: {type(state)}\")\n",
    "\n",
    "    # Extract the state if it's a tuple and reshape if necessary\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]  # Extract the first element if it's a tuple\n",
    "\n",
    "    state = np.reshape(state, [1, state_size])  # Reshape state to match the expected input shape\n",
    "\n",
    "    for time in range(500):  # Limit the episode to 500 time steps\n",
    "        action = act(state)  # Choose an action based on the current state\n",
    "        \n",
    "        # Unpack 5 values returned by env.step(action)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        # Compute the custom reward based on the next state\n",
    "        reward = custom_reward(next_state) if not done else -10\n",
    "\n",
    "        # Reshape next_state if necessary\n",
    "        if isinstance(next_state, tuple):\n",
    "            next_state = next_state[0]  # Extract the first element if it's a tuple\n",
    "\n",
    "        next_state = np.reshape(next_state, [1, state_size])  # Reshape next state to match input shape\n",
    "\n",
    "        # Store the experience in memory\n",
    "        remember(state, action, reward, next_state, done)\n",
    "        state = next_state  # Update the current state\n",
    "\n",
    "        if done or truncated:  # If the episode is done, break out of the loop\n",
    "            print(f\"episode: {e}/{episodes}, score: {time}, e: {epsilon:.2}\")\n",
    "            break\n",
    "\n",
    "        if len(memory) > batch_size:  # If there are enough samples in memory, train the model\n",
    "            replay(batch_size)  # Train the model with a batch of experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Define a custom reward function based on the cart position and pole angle\n",
    "def custom_reward(state):\n",
    "    # Extract state variables: x (cart position), x_dot (cart velocity), theta (pole angle), theta_dot (pole angular velocity)\n",
    "    x, x_dot, theta, theta_dot = state\n",
    "    \n",
    "    # Custom reward function: Encourage the agent to keep the cart near the center and the pole upright\n",
    "    reward = (1 - abs(x) / 2.4) + (1 - abs(theta) / 0.20948)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "episodes = 2  # Number of episodes to run\n",
    "\n",
    "# Train the model with the custom reward function\n",
    "for e in range(episodes): \n",
    "    state = env.reset()  # Reset the environment\n",
    "\n",
    "    # Print the state structure for debugging\n",
    "    print(f\"State: {state}, State Type: {type(state)}\")\n",
    "\n",
    "    # Extract the state if it's a tuple and reshape if necessary\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]  # Extract the first element if it's a tuple\n",
    "\n",
    "    state = np.reshape(state, [1, state_size])  # Reshape state to match the expected input shape\n",
    "\n",
    "    for time in range(500):  # Limit the episode to 500 time steps\n",
    "        action = act(state)  # Choose an action based on the current state\n",
    "        \n",
    "        # Unpack 5 values returned by env.step(action)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        # Compute the custom reward based on the next state\n",
    "        reward = custom_reward(next_state) if not done else -10\n",
    "\n",
    "        # Reshape next_state if necessary\n",
    "        if isinstance(next_state, tuple):\n",
    "            next_state = next_state[0]  # Extract the first element if it's a tuple\n",
    "\n",
    "        next_state = np.reshape(next_state, [1, state_size])  # Reshape next state to match input shape\n",
    "\n",
    "        # Store the experience in memory\n",
    "        remember(state, action, reward, next_state, done)\n",
    "        state = next_state  # Update the current state\n",
    "\n",
    "        if done or truncated:  # If the episode is done, break out of the loop\n",
    "            print(f\"episode: {e}/{episodes}, score: {time}, e: {epsilon:.2}\")\n",
    "            break\n",
    "\n",
    "        if len(memory) > batch_size:  # If there are enough samples in memory, train the model\n",
    "            replay(batch_size)  # Train the model with a batch of experiences\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion \n",
    "\n",
    "Congratulations on completing this lab!  In this lab, you explored various strategies to enhance the performance of the Q-Learning agent, such as experimenting with different network architectures, implementing adaptive exploration rates, and customizing the reward function. These variations help reinforce your understanding of the Q-Learning algorithm's flexibility and the impact of different hyperparameters and strategies on the learning process.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skills Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "c7aa20b200c76f649b9de38f08b4d27f83eff96f5933b3756055c6d8d0fb5867"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
